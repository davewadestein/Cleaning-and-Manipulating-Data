{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating and Cleaning Data\n",
    "\n",
    "* Real-world data is messy!\n",
    "* You'll likely need to combine several data sources to get the data you actually want\n",
    "  * Data from those sources will be incomplete\n",
    "  * ...and it will likely not be formatted in exactly the way you want \n",
    "* ~ 80% of a Data Scientist's time is spent manipulating and cleaning data\n",
    "  * \"Data Wrangling\"\n",
    "* Python is commonly used for data wrangling (and really, all aspects of Data Science)\n",
    "\n",
    "## __`pandas`__ (https://pandas.pydata.org/)\n",
    "* de facto data wrangling tool for Python\n",
    "* adds two new useful datatypes\n",
    "  * __`Series`__\n",
    "    * like a Python list but with editable indices for each element\n",
    "  * __`DataFrame`__\n",
    "    * 2-D, like an Excel spreadsheet\n",
    "\n",
    "## Exploring `DataFrame` Information\n",
    "\n",
    "> **Learning goal:** Be able to obtain general information about data stored in DataFrames\n",
    "\n",
    "* If the data in your __`DataFrame`__ has 60,000 rows and 400 columns, how do you even begin to get a sense of what you're working with?\n",
    "* __`pandas`__ provides convenient tools\n",
    "* in order to explore __`pandas`__, we'll import __`scikit-learn`__ (a popular machine learning library) and use a well-known dataset–the **Iris** data set\n",
    "  * Ronald Fisher, 1936\n",
    "  * [\"The use of multiple measurements in taxonomic problems\"](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __`DataFrame.info`__\n",
    "* let's take a look at this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __`DataFrame.head`__\n",
    "* let's see the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* by default, __`DataFrame.head`__ returns the first five rows of a __`DataFrame`__\n",
    "* in the code cell below, can you figure out how to get it to show more...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Consult the documentation by using iris_df.head?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.tail`\n",
    "* __`DataFrame.tail`__ returns the last five rows of a __`DataFrame`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In practice, it is useful to be able to easily examine the first few or last few rows, particularly when looking for outliers in ordered datasets.`\n",
    "\n",
    "> **Takeaway:** Just by looking at the metadata of a DataFrame or the first and last few rows, you can get an immediate idea about size, shape, and content of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "> **Learning goal:** Know how to replace or remove null values from DataFrames\n",
    "\n",
    "* Missing values occur all the time\n",
    "* Handling missing data involves subtle tradeoffs that can affect your final analysis and real-world outcomes\n",
    "\n",
    "## How Pandas Handles Missing Values\n",
    "* __`NaN`__ (\"not a number)\n",
    "  * special value that is part of the IEEE floating-point specification and it is only used to indicate missing floating-point values\n",
    "* __`None`__–a special Python value similar to __`NULL`__ or __`nil`__ in other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `None`: non-float missing data\n",
    "> **Rabbit Hole**\n",
    "* __`pandas`__ is built on top of __`numpy`__\n",
    "  * a numerical package for Python (...because Python is a terrible language for numerics)\n",
    "  * adds an _array_ type to Python\n",
    "  * very persnickety when it comes to types\n",
    "  * if you feed non-numeric data to __numpy__, it will be treated as type __`object`__\n",
    "* because __`None`__ comes from Python, it can only be used in __`numpy`__ arrays that are of type __`object`__\n",
    "* let's see an example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example1 = np.array([2, 4, 6, 8])\n",
    "example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example1 = np.array([2, None, 6, 8])\n",
    "example1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    "Because `None` essentially drags __`pandas`__ datatypes back into the world of vanilla Python, using aggregations like __`sum()`__ or __`min()`__ on arrays that contain a __`None`__ value will generally produce an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway**: Addition (and other operations) between integers and `None` values is undefined, which can limit what you can do with datasets that contain them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `NaN`: missing float values\n",
    "* In contrast to `None`, __`numpy`__ (and therefore pandas) supports __`NaN`__\n",
    "  * The bad news is that any arithmetic performed on `NaN` always results in __`NaN`__ and cannot be usefully compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The good news: aggregations run on arrays with __`NaN`__ in them don't produce errors\n",
    "* The bad news: the results are not uniformly useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2 = np.array([2, np.nan, 6, 8]) \n",
    "example2.sum(), example2.min(), example2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if you add np.nan and None together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: __`NaN`__ is just for missing floating-point values; there is no __`NaN`__ equivalent for integers, strings, or Booleans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `NaN` and `None`: null values in pandas\n",
    "\n",
    "* even though __`NaN`__ and __`None`__ can behave somewhat differently, __`pandas`__ is nevertheless built to handle them interchangeably\n",
    "* to see this, consider a __`Series`__ of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_series = pd.Series([3, 5, 7], dtype=int)\n",
    "int_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set an element of int_series equal to None.\n",
    "# How does that element show up in the Series?\n",
    "# What is the dtype of the Series?\n",
    "int_series[0] = None\n",
    "int_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when upcasting datatypes to establish data homogeneity in __`Series`__ and __`DataFrame`__s, __`pandas`__ will willingly switch missing values between __`None`__ and __`NaN`__\n",
    "  * it can be helpful to think of __`None`__ and __`NaN`__ as two different flavors of \"null\"\n",
    "  * ...some of the core __`pandas`__ functions you will use to deal with missing values reflect this in their names:\n",
    "    * __`isnull()`__\n",
    "    * __`notnull()`__\n",
    "    * __`dropna()`__\n",
    "    * __`fillna()`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Null Values\n",
    "* both __`isnull()`__ and __`notnull()`__ are your primary ways of detecting null data\n",
    "* these functions return \"Boolean masks\", i.e., an array (or __`Series`__) of __`True`__ or __`False`__ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example3 = pd.Series([0, np.nan, '', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example3.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* look closely at the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running example3[example3.notnull()].\n",
    "# Before you do so, what do you expect to see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Null Values\n",
    "* __`pandas`__ provides a convenient means to remove null values from __`Series`__ and __`DataFrame`__s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example3 = example3.dropna()\n",
    "example3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** that this should look like your output from __`example3[example3.notnull()]`__ \n",
    "* The difference here is that, rather than just indexing on the masked values, __`dropna()`__ has removed those missing values from the __`example3`__\n",
    "\n",
    "* because __`DataFrame`__s have two dimensions, they afford more options for dropping data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4 = pd.DataFrame([[1,      np.nan, 7], \n",
    "                         [2,      5,      8], \n",
    "                         [np.nan, 6,      9]])\n",
    "example4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Did you notice that pandas upcast two of the columns to floats to accommodate the __`NaN`__s?\n",
    "\n",
    "* You cannot drop a single value from a __`DataFrame`__, you have to drop full rows or columns\n",
    "  * rows generally represent observations, i.e., 1 iris plant\n",
    "  * columns generally represent a feature or attribute of the input, e.g, \"sepal length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4.dropna(axis='columns') # or axis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** this can drop a lot of data that you might want to keep, particularly in smaller datasets\n",
    "  * What if you just want to drop rows or columns that contain several or even just all null values?\n",
    "    * Specify those setting in with the __`how`__ and __`thresh`__ parameters  \n",
    "      * By default, __`how='any'`__ \n",
    "      * let's look at the docs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4.dropna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4[3] = np.nan\n",
    "example4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How might you go about dropping just column 3?\n",
    "# Hint: remember that you will need to supply both the axis parameter and the how parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __`thresh=`__ gives you finer-grained control: you set the number of *non-null* values that a row or column needs to have in order to be kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4.dropna(axis='rows', thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Null Values\n",
    "* it sometimes make more sense to fill null values with valid ones rather than drop them\n",
    "* __`fillna()`__ returns a copy of the __`Series`__ or __`DataFrame`__ with missing values replaced with one of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example5 = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "example5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fill all of the null entries with a single value, such as `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example5.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if you try to fill null values with a string, like ''?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **forward-fill** null values, i.e., use last valid value to fill a null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example5.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ... or **back-fill** to propagate the next valid value backward to fill a null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example5.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* as you might guess, works the same with __`DataFrame`__s, but you can also specify an __`axis`__ along which to fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** when a previous value is not available for forward-filling, the null remains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What output does example4.fillna(method='bfill', axis=1) produce?\n",
    "# What about example4.fillna(method='ffill') or example4.fillna(method='bfill')?\n",
    "# Can you think of a longer code snippet to write that can fill all of the null\n",
    "# values in example4?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* you can (and probably should be) more creative about when using __`fillna()`__\n",
    "* let's look at __`example4`__ again and this time fill missing values with the average of all of the values in the __`DataFrame`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example4.fillna(example4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** column 3 is still all NaNs\n",
    " * default direction is to fill values row-wise\n",
    "\n",
    "> **Takeaway:**\n",
    "* multiple ways to deal with missing values in datasets\n",
    "* the strategy you choose (removing, replacing, or even how you replace) should be dictated by the particulars of that data\n",
    "* you will develop a better sense of how to deal with missing values the more you handle and interact with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicate data\n",
    "\n",
    "> **Learning goal:** identify and removing duplicate values from __`DataFrames`__\n",
    "* in addition to missing data, you will often encounter duplicated data in real-world datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying duplicates: __`duplicated()`__\n",
    "* returns a Boolean mask indicating whether an entry in a __`DataFrame`__ is a duplicate of an earlier one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example6 = pd.DataFrame({'letters': ['A','B'] * 2 + ['B'],\n",
    "                         'numbers': [1, 2, 1, 3, 3]})\n",
    "example6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example6.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping duplicates: `drop_duplicates`\n",
    "* returns a copy of the data for which all of the duplicated values are __`False`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example6.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* both __`duplicated`__ and __`drop_duplicates`__ default to all columns but you may specify they examine only a subset of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example6.drop_duplicates(['letters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Takeaway:** Removing duplicate data is an essential part of almost every data-science project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining datasets: merge and join\n",
    "\n",
    "> **Learning goal:** general knowledge of the various ways to combine __`DataFrame`__s\n",
    "\n",
    "* The most interesting analyses often come from data combined from multiple sources\n",
    "* __`pandas`__ provides several methods of merging and joining datasets to make this necessary job easier\n",
    " * __`merge()`__ connects rows in __`DataFrame`__s based on one or more keys\n",
    " * __`pandas.concat`__ concatenates or “stacks” together objects along an axis\n",
    " * __`combine_first`__, which we won't cover, splices together overlapping data to fill in missing values in one object with values from another)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories of Joins\n",
    "\n",
    "* __`merge`__ carries out several types of joins\n",
    " * *one-to-one*\n",
    " * *many-to-one*\n",
    " * *many-to-many*\n",
    "* the same function call implements all of them and we will examine all three\n",
    "* let's start with one-to-one joins because they are generally the simplest example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-to-one joins\n",
    "* consider combining two __`DataFrame`__s that contain different information about the same employees in a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Gary', 'Stu', 'Mary', 'Sue'],\n",
    "                    'group': ['Accounting', 'Marketing', 'Marketing', 'HR']})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'employee': ['Mary', 'Stu', 'Gary', 'Sue'],\n",
    "                    'hire_date': [2008, 2012, 2017, 2018]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we'll combine this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** __`pandas`__ joined on the __`employee`__ column because it was the only common column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-one joins\n",
    "* like a one-to-one join except that one of the two key columns contains duplicate entries\n",
    "* result of such a join will preserve those duplicate entries as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'group': ['Accounting', 'Marketing', 'HR'],\n",
    "                    'supervisor': ['Carlos', 'Giada', 'Stephanie']})\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df3, df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* result contains an additional column for __`supervisor`__\n",
    "  * that column has an extra occurence of 'Giada' that did not occur in __`df4`__ because more than one employee works in the 'Marketing' group\n",
    "\n",
    "> **Note:** we didn’t specify which column to join on\n",
    "* by default, __`merge`__ uses overlapping column names as the keys\n",
    "* However, that can be ambiguous; good practice dictates we explicitly specify on which key to join using the `on` parameter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df3, df4, on='group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-many joins\n",
    "* What if the key columns in both __`DataFrame`__s contain duplicates?\n",
    "* That gives you a many-to-many join..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Marketing', 'Marketing', 'HR', 'HR'],\n",
    "                    'core_skills': ['math', 'spreadsheets', 'writing', 'communication',\n",
    "                               'spreadsheets', 'organization']})\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df5, on='group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `left_on` and `right_on` keywords\n",
    "* What if two datasets share no column names?\n",
    "* For example, what if the employee name is called as __`name`__ rather than __`employee`__? \n",
    "* ...use __`left_on`__ and __`right_on`__ keywords in order to specify the column names on which to join..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n",
    "                    'office': [736, 7847, 1201, 906]})\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df6, left_on=\"employee\", right_on=\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the documentation, can you figure out how to use .drop() to get rid of the 'name' column?\n",
    "# Hint: You will need to supply two parameters to .drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `left_index` and `right_index` keywords\n",
    "* sometimes it's more advantageous to merge on an index rather than a column\n",
    "* __`left_index`__ and __`right_index`__ keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1a = df1.set_index('employee')\n",
    "df1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2a = df2.set_index('employee')\n",
    "df2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge on the index, specify the `left_index` and `right_index` parameters in `merge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1a, df2a, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if you specify only left_index or right_index?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* you can also use the __`join`__ method, which produces the same effect but merges on indices by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1a.join(df2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* you can also mix and match __`left_index`__/__`right_index`__ with __`right_on`__/__`left_on`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1a, df6, left_index=True, right_on='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set arithmetic for joins\n",
    "* let's return to many-to-many joins\n",
    "* unique to them is the *arithmetic* of the join, specifically the set arithmetic we use for the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'group': ['Engineering', 'Marketing', 'Sales'],\n",
    "                    'core_skills': ['math', 'writing', 'communication']})\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df5, on='group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** we have only two entries in the result, because we merged on __`group`__ and 'Marketing' was the only entry that appeared in the `group` column of both `DataFrame`s\n",
    "\n",
    "* we end up with the *intersection* of both __`DataFrame`__s\n",
    "  * this is know as \"inner join\" in the database world and it is the default setting for __`merge`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df5, on='group', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the complement of the inner join is the outer join, which returns the *union* of the two `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keyword for perfoming an outer join is how='outer'. How would you perform it?\n",
    "# What do you expect the output of an outer join of df1 and df5 to be?\n",
    "pd.merge(df1, df5, on='group', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** in the result, not every row in __`df1`__ and __`df5`__ had a value that corresponds to the union of the key values (the 'group' column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inner and outer joins are not the only options\n",
    "* **left** join returns all of the rows in the first (left-side) `DataFrame` supplied to `merge` along with rows from the other `DataFrame` that match up with the left-side key values (and `NaNs` rows with respective values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df5, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the right merge between df1 and df5.\n",
    "# What do you expect to see?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `suffixes` keyword: dealing with conflicting column names\n",
    "* what if join two datasets with conflicting column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n",
    "                    'rank': [1, 2, 3, 4]})\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n",
    "                    'rank': [3, 1, 4, 2]})\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df7, df8, on='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Each column name in a __`DataFrame`__ must be unique, so when two joined __`DataFrame`__s share column names (aside from the column serving as the key), the __`merge`__ function appends the suffix __`_x`__ or __`_y`__ to conflicting column names to make them unique\n",
    "* you can specify a custom suffix for __`merge`__ to append through the __`suffixes`__ keyword..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df7, df8, on='name', suffixes=['_left', '_right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation in NumPy\n",
    "Concatenation in pandas is built off of the concatenation functionality for NumPy arrays. Here is what NumPy concatenation looks like:\n",
    " - For one-dimensional arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "np.concatenate([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - For two-dimensional arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1, 2],\n",
    "     [3, 4]]\n",
    "np.concatenate([x, x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `axis=1` parameter makes the concatenation occur along columns rather than rows. Concatenation in pandas looks similar to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation in pandas\n",
    "\n",
    "Pandas has a function, `pd.concat()` that can be used for a simple concatenation of `Series` or `DataFrame` objects in similar manner to `np.concatenate()` with ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series(['a', 'b', 'c'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['d', 'e', 'f'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also concatenates higher-dimensional objects, such as ``DataFrame``s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.DataFrame({'A': ['a', 'c'],\n",
    "                    'B': ['b', 'd']})\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df9, df9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `pd.concat` has preserved the indexing even though that means that it has been duplicated. You can have the results re-indexed (and avoid potential confusion down the road) like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df9, df9], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `pd.concat` concatenates row-wise within the `DataFrame` (that is, `axis=0` by default). You can specify the axis along which to concatenate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df9, df9], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while pandas will display this without error, you will get an error message if you try to assign this result as a new `DataFrame`. Column names in `DataFrame`s must be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation with joins\n",
    "Just as you did with merge above, you can use inner and outer joins when concatenating `DataFrame`s with different sets of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame({'A': ['a', 'd'],\n",
    "                     'B': ['b', 'e'],\n",
    "                     'C': ['c', 'f']})\n",
    "df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = pd.DataFrame({'B': ['u', 'x'],\n",
    "                     'C': ['v', 'y'],\n",
    "                     'D': ['w', 'z']})\n",
    "df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df10, df11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, the default join for this is an outer join and entries for which no data is available are filled with `NaN` values. You can also do an inner join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df10, df11], join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to directly specify the index of the remaininig colums using the `join_axes` argument, which takes a list of index objects. Here, we will specify that the returned columns should be the same as those of the first input (`df10`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df10, df11], join_axes=[df10.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `append()`\n",
    "\n",
    "Because direct array concatenation is so common, ``Series`` and ``DataFrame`` objects have an ``append`` method that can accomplish the same thing in fewer keystrokes. For example, rather than calling ``pd.concat([df9, df9])``, you can simply call ``df9.append(df9)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.append(df9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important point**: Unlike the `append()` and `extend()` methods of Python lists, the `append()` method in pandas does not modify the original object. It instead creates a new object with the combined data.\n",
    "\n",
    "> **Takeaway:** A large part of the value you can provide as a data scientist comes from connecting multiple, often disparate datasets to find new insights. Learning how to join and merge data is thus an essential part of your skill set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory statistics and visualization\n",
    "\n",
    "> **Learning goal:** By the end of this subsection, you should be familiar with some of the ways to visually explore the data stored in `DataFrame`s.\n",
    "\n",
    "Often when probing a new data set, it is invaluable to get high-level information about what the dataset holds. Earlier in this section we discussed using methods such as `DataFrame.info`, `DataFrame.head`, and `DataFrame.tail` to examine some aspects of a `DataFrame`. While these methods are critical, they are on their own often insufficient to get enough information to know how to approach a new dataset. This is where exploratory statistics and visualizations for datasets come in.\n",
    "\n",
    "To see what we mean in terms of gaining exploratory insight (both visually and numerically), let's dig into one of the the datasets that come with the scikit-learn library, the Boston Housing Dataset (though you will load it from a CSV file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/housing_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains information collected from the U.S Census Bureau concerning housing in the area of Boston, Massachusetts and was first published in 1978. The dataset has 14 columns:\n",
    " - **CRIM**:     Per-capita crime rate by town\n",
    " - **ZN**:       Proportion of residential land zoned for lots over 25,000 square feet\n",
    " - **INDUS**:    Proportion of non-retail business acres per town\n",
    " - **CHAS**:     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    " - **NOX**:      Nitric oxides concentration (parts per 10 million)\n",
    " - **RM**:       Average number of rooms per dwelling\n",
    " - **AGE**:      Proportion of owner-occupied units built prior to 1940\n",
    " - **DIS**:      Weighted distances to five Boston employment centres\n",
    " - **RAD**:      Index of accessibility to radial highways\n",
    " - **TAX**:      Full-value property-tax rate per \\$10,000\n",
    " - **PTRATIO**:  Pupil-teacher ratio by town\n",
    " - **LSTAT**:    Percent of lower-status portion of the population\n",
    " - **MEDV**:     Median value of owner-occupied homes in \\$1,000s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first methods we can use to better understand this dataset is `DataFrame.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 506 rows and 13 columns.\n",
    "\n",
    "To get a better idea of the contents of each column we can use `DataFrame.describe`, which returns the maximum value, minimums value, mean, and standard deviation of numeric values in each columns, in addition to the quartiles for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because dataset can have so many columns in them, it can often be useful to transpose the results of `DataFrame.describe` to better use them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also examine specific descriptive statistics for columns without having to invoke `DataFrame.describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MEDV'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MEDV'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AGE'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find the maximum value in df['AGE'].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other information that you will often want to see is the relationship between different columns. You do this with the `DataFrame.groupby` method. For example, you could examine the average MEDV (median value of owner-occupied homes) for each value of AGE (proportion of owner-occupied units built prior to 1940):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['AGE'])['MEDV'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to find the median value for AGE for each value of MEDV.\n",
    "df.groupby(['MEDV'])['AGE'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply a lambda function to each element of a `DataFrame` column by using the `apply` method. For example, say you wanted to create a new column that flagged a row if more than 50 percent of owner-occupied homes were build before 1940:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AGE_50'] = df['AGE'].apply(lambda x: x>50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once applied, you also see how many values returned true and how many false by using the `value_counts` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AGE_50'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also examine figures from the groupby statement you created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['AGE_50'])['MEDV'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also group by more than one variable, such AGE_50 (the one you just created), CHAS (whether a town is on the Charles River), and RAD (an index measuring access to the Boston-area radial highways), and then evaluate each group for the average median home price in that group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_twovar=df.groupby(['AGE_50','RAD','CHAS'])['MEDV'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then see what values are in this stacked group of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_twovar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to analyze these results in a little depth. The first row reports that communities with less the half of houses built before 1940, with a highway-access index of 1, and that are not situated on the Charles River have a mean house price of \\\\$24,667 (1970s dollars); the next row shows that for communities similar to the first row except for being located on the Charles River have a mean house price of \\\\$50,000.\n",
    "\n",
    "One insight that pops out from continuing down this is that, all else being equal, being located next to the Charles River can significantly increase the value of newer housing stock. The story is more ambiguous for communities dominated by older houses: proximity to the Charles significantly increases home prices in one community (and that one presumably farther away from the city); for all others, being situated on the river either provided a modest increase in value or actually decreased mean home prices.\n",
    "\n",
    "While groupings like this can be a great way to begin to interrogate your data, you might not care for the 'tall' format it comes in. In that case, you can unstack the data into a \"wide\" format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_twovar.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How could you use groupby to get a sense of the proportion \n",
    "# of residential land zoned for lots over 25,000 sq.ft., \n",
    "# the proportion of non-retail business acres per town, \n",
    "# and the distance of towns from employment centers in Boston?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is also often valuable to know how many unique values a column has in it with the `nunique` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CHAS'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complementary to that, you will also likely want to know what those unique values are, which is where the `unique` method helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CHAS'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `value_counts` method to see how many of each unique value there are in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CHAS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can easily plot a bar graph to visually see the breakdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df['CHAS'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the IPython magic command `%matplotlib inline` enables you to view the chart inline.\n",
    "\n",
    "Let's pull back to the dataset as a whole for a moment. Two major things that you will look for in almost any dataset are trends and relationships. A typical relationship between variables to explore is the Pearson correlation, or the extent to which two variables are linearly related. The `corr` method will show this in table format for all of the columns in a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you just wanted to look at the correlations between all of the columns and just one variable? Let's examine just the correlation between all other variables and the percentage of owner-occupied houses build before 1940 (AGE). We will do this by accessing the column by index number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(method='pearson')\n",
    "corr_with_homevalue = corr.iloc[-1]\n",
    "corr_with_homevalue[corr_with_homevalue.argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the correlations arranged in descending order, it's easy to start to see some patterns. Correlating AGE with a variable we created from AGE is a trivial correlation. However, it is interesting to note that the percentage of older housing stock in communities strongly correlates with air pollution (NOX) and the proportion of non-retail business acres per town (INDUS); at least in 1978 metro Boston, older towns are more industrial.\n",
    "\n",
    "Graphically, we can see the correlations using a heatmap from the Seaborn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(df.corr(),cmap=sns.cubehelix_palette(20, light=0.95, dark=0.15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms are another valuable tool for investigating your data. For example, what is the overall distribution of prices of owner-occupied houses in the Boston area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default bin size for the matplotlib histogram (essentially big of buckets of percentages that you include in each histogram bar in this case) is pretty large and might mask smaller details. To get a finer-grained view of the AGE column, you can manually increase the number of bins in the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['MEDV'],bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn has a somewhat more attractive version of the standard matplotlib histogram: the distribution plot. This is a combination histogram and kernel density estimate (KDE) plot (essentially a smoothed histogram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly used plot is the Seaborn jointplot, which combines histograms for two columns along with a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(df['RM'], df['MEDV'], kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, many of the dots print over each other. You can help address this by adding some alpha blending, a figure that sets the transparency for the dots so that concentrations of them drawing over one another will be apparent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(df['RM'], df['MEDV'], kind='scatter', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see patterns in your data is with a two-dimensional KDE plot. Darker colors here represent a higher concentration of data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df['RM'], df['MEDV'], shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that while the KDE plot is very good at showing concentrations of data points, finer structures like linear relationships (such as the clear relationship between the number of rooms in homes and the house price) are lost in the KDE plot.\n",
    "\n",
    "Finally, the pairplot in Seaborn allows you to see scatterplots and histograms for several columns in one table. Here we have played with some of the keywords to produce a more sophisticated and easier to read pairplot that incorporates both alpha blending and linear regression lines for the scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['RM', 'AGE', 'LSTAT', 'DIS', 'MEDV']], kind=\"reg\", plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Visualization is the start of the really cool, fun part of data science. So play around with these visualization tools and see what you can learn from the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Takeaway:** An old joke goes: “What does a data scientist seen when they look at a dataset? A bunch of numbers.” There is more than a little truth in that joke. Visualization is often the key to finding patterns and correlations in your data. While visualization cannot often deliver precise results, it can point you in the right direction to ask better questions and efficiently find value in the data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
